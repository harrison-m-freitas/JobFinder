# √âpica: Sistema de Coleta e An√°lise de Vagas de Tecnologia

## üìã Descri√ß√£o Geral

Desenvolver um sistema automatizado de web scraping para coletar, armazenar e processar ofertas de trabalho na √°rea de tecnologia de m√∫ltiplas fontes (job boards, sites de empresas, redes profissionais). O sistema deve consolidar dados de vagas preferencialmente no Brasil e remotas, armazenando informa√ß√µes detalhadas para posterior an√°lise de intelig√™ncia de mercado.

## üéØ Objetivos

- Automatizar a coleta de vagas de tecnologia de m√∫ltiplas fontes
- Centralizar informa√ß√µes em banco de dados estruturado
- Coletar dados ricos para an√°lise de mercado e intelig√™ncia competitiva
- Possibilitar an√°lises de tend√™ncias salariais, skills demandadas, empresas contratantes
- Monitorar oportunidades em tempo real

## üîç Escopo

### √Åreas de Tecnologia Alvo
- Desenvolvimento de Software (Junior, Mid-level, Senior)
- Cybersecurity / Seguran√ßa da Informa√ß√£o
- Pentester / Ethical Hacking
- QA / Quality Assurance
- DevOps / SRE
- Data Science / Machine Learning
- Cloud Computing
- Mobile Development
- Frontend / Backend / Full Stack

### Localiza√ß√£o
- **Prim√°rio**: Brasil (todas as regi√µes)
- **Secund√°rio**: Vagas remotas internacionais
- **Idiomas**: Portugu√™s e Ingl√™s

## üìä Fontes de Dados (Sources/Providers)

### Job Boards Generalistas
- LinkedIn Jobs
- Indeed Brasil
- Glassdoor
- Catho
- InfoJobs
- Gupy
- Workana
- Trampos.co

### Plataformas Especializadas em Tech
- GeekHunter
- Revelo
- Hired
- Stack Overflow Jobs
- GitHub Jobs
- AngelList (Wellfound)
- Remote.co
- We Work Remotely
- RemoteOK

### Sites de Empresas (Career Pages)
**Big Tech**
- Google Careers
- Amazon Jobs
- Microsoft Careers
- Meta Careers
- Apple Jobs

**Empresas Brasileiras**
- Banco Inter Carreiras
- Mercado Livre Carreiras
- Nubank Careers
- Stone Careers
- iFood Careers
- Loggi Careers
- QuintoAndar Careers
- PicPay Careers
- B2W Digital

**Consultorias / Tech Companies**
- Thoughtworks
- CI&T
- TOTVS
- Stefanini
- Accenture Brasil

## üíæ Modelagem de Dados Sugerida

### Tabela: `jobs`
```
- id (PK, UUID)
- external_id (identificador √∫nico da fonte)
- title (t√≠tulo da vaga)
- company_id (FK para companies)
- source (origem da vaga: linkedin, indeed, etc)
- source_url (URL original da vaga)
- description (descri√ß√£o completa em texto)
- description_html (descri√ß√£o original em HTML)
- seniority_level (junior, mid, senior, staff, principal)
- employment_type (full-time, part-time, contract, internship)
- location_city
- location_state
- location_country
- is_remote (boolean)
- remote_type (fully_remote, hybrid, on_site)
- salary_min (decimal)
- salary_max (decimal)
- salary_currency
- salary_period (monthly, annual)
- posted_date (data de publica√ß√£o)
- expires_date (data de expira√ß√£o)
- applicants_count (n√∫mero de candidatos)
- status (active, expired, filled, removed)
- scraped_at (timestamp da coleta)
- updated_at (√∫ltima atualiza√ß√£o)
- created_at
```

### Tabela: `companies`
```
- id (PK, UUID)
- name
- normalized_name (nome normalizado para matching)
- website
- industry (setor/ind√∫stria)
- size (startup, small, medium, large, enterprise)
- employee_count_min
- employee_count_max
- location_hq (sede principal)
- description
- logo_url
- linkedin_url
- glassdoor_url
- created_at
- updated_at
```

### Tabela: `job_skills`
```
- id (PK)
- job_id (FK)
- skill_id (FK)
- is_required (boolean)
- proficiency_level (basic, intermediate, advanced)
```

### Tabela: `skills`
```
- id (PK)
- name (ex: Python, React, Kubernetes)
- category (language, framework, tool, platform, methodology)
- normalized_name
```

### Tabela: `job_benefits`
```
- id (PK)
- job_id (FK)
- benefit_type (health_insurance, meal_voucher, education, stock_options, etc)
- description
```

### Tabela: `scraping_logs`
```
- id (PK)
- source
- execution_date
- jobs_found
- jobs_inserted
- jobs_updated
- errors_count
- execution_time_seconds
- status (success, partial_failure, failure)
- error_details (JSON)
```

## üõ†Ô∏è Stack Tecnol√≥gica Recomendada

### Core
- **Linguagem**: Python 3.10+
- **Web Scraping**:
  - Scrapy (framework robusto)
  - BeautifulSoup4 (parsing HTML)
  - Selenium (sites com JavaScript)
  - Playwright (alternativa moderna ao Selenium)
- **HTTP Requests**: httpx, requests
- **APIs**: Quando dispon√≠veis (LinkedIn API, Indeed API)

### Banco de Dados
- **Prim√°rio**: PostgreSQL (relacional, JSONB para dados flex√≠veis)
- **Alternativas**: MySQL, MongoDB (se preferir NoSQL)
- **ORM**: SQLAlchemy ou Django ORM

### Infraestrutura
- **Agendamento**: Apache Airflow, Celery + Redis, ou Cron
- **Containeriza√ß√£o**: Docker + Docker Compose
- **Queue**: RabbitMQ ou Redis (para jobs ass√≠ncronos)

### Processamento de Dados
- **NLP**: spaCy, NLTK (extra√ß√£o de skills, an√°lise de texto)
- **Matching**: FuzzyWuzzy (deduplicate vagas/empresas)
- **Data Cleaning**: pandas

### Monitoramento
- **Logs**: Python logging + ELK Stack ou Grafana Loki
- **M√©tricas**: Prometheus + Grafana
- **Alertas**: Email, Slack, Telegram

## üìù Tarefas e Subtarefas

### 1. Setup e Arquitetura (Story Points: 8)
- [x] 1.1 Definir arquitetura do sistema
- [x] 1.2 Configurar ambiente de desenvolvimento
- [x] 1.3 Setup Docker e Docker Compose
- [x] 1.4 Escolher e configurar banco de dados
- [x] 1.5 Criar estrutura de projeto Python
- [x] 1.6 Configurar sistema de versionamento e CI/CD

### 2. Modelagem e Banco de Dados (Story Points: 13)
- [x] 2.1 Definir modelagem completa de dados
- [x] 2.2 Criar migrations do banco de dados
- [x] 2.3 Implementar models/ORM
- [x] 2.4 Criar √≠ndices para otimiza√ß√£o
- [x] 2.5 Implementar sistema de backup
- [x] 2.6 Desenvolver scripts de seed/fixtures para testes

### 3. Core do Sistema de Scraping (Story Points: 21)
- [x] 3.1 Criar classe base abstrata para scrapers
- [x] 3.2 Implementar sistema de detec√ß√£o de robots.txt
- [x] 3.3 Desenvolver rate limiting e delays
- [x] 3.4 Implementar rota√ß√£o de User-Agents
- [x] 3.5 Sistema de retry com backoff exponencial
- [x] 3.6 Implementar proxy rotation (se necess√°rio)
- [x] 3.7 Sistema de cache para evitar reprocessamento
- [x] 3.8 Deduplica√ß√£o de vagas

### 4. Scrapers - Job Boards Generalistas (Story Points: 34)
- [ ] 4.1 Scraper: LinkedIn Jobs
- [ ] 4.2 Scraper: Indeed Brasil
- [ ] 4.3 Scraper: Glassdoor
- [ ] 4.4 Scraper: Catho
- [ ] 4.5 Scraper: InfoJobs
- [ ] 4.6 Scraper: Gupy
- [ ] 4.7 Scraper: Trampos.co

### 5. Scrapers - Plataformas Tech (Story Points: 21)
- [ ] 5.1 Scraper: GeekHunter
- [ ] 5.2 Scraper: GitHub Jobs
- [ ] 5.3 Scraper: Stack Overflow Jobs
- [ ] 5.4 Scraper: Remote.co / WeWorkRemotely
- [ ] 5.5 Scraper: AngelList

### 6. Scrapers - Career Pages (Story Points: 34)
- [ ] 6.1 Template gen√©rico para career pages
- [ ] 6.2 Scraper: Google Careers
- [ ] 6.3 Scraper: Amazon Jobs
- [ ] 6.4 Scraper: Mercado Livre
- [ ] 6.5 Scraper: Nubank
- [ ] 6.6 Scraper: Banco Inter
- [ ] 6.7 Scraper: iFood
- [ ] 6.8 Scraper: Stone
- [ ] 6.9 Adicionar mais 5-10 empresas relevantes

### 7. Processamento e Enriquecimento de Dados (Story Points: 21)
- [ ] 7.1 Parser de descri√ß√µes de vagas (extra√ß√£o de skills)
- [ ] 7.2 Extra√ß√£o de requisitos (anos de experi√™ncia, idiomas)
- [ ] 7.3 Normaliza√ß√£o de t√≠tulos de vagas
- [ ] 7.4 Detec√ß√£o de n√≠vel de senioridade
- [ ] 7.5 Extra√ß√£o e parsing de sal√°rios
- [ ] 7.6 Normaliza√ß√£o de nomes de empresas
- [ ] 7.7 Geocoding de localiza√ß√µes
- [ ] 7.8 Extra√ß√£o de benef√≠cios

### 8. Sistema de Agendamento (Story Points: 13)
- [ ] 8.1 Configurar Airflow ou alternativa
- [ ] 8.2 Criar DAGs para cada fonte
- [ ] 8.3 Definir frequ√™ncia de coleta por fonte
- [ ] 8.4 Implementar sistema de prioriza√ß√£o
- [ ] 8.5 Configurar alertas de falhas

### 9. API e Interface de Dados (Story Points: 21)
- [ ] 9.1 Criar API REST (FastAPI/Flask)
- [ ] 9.2 Endpoints de consulta de vagas
- [ ] 9.3 Filtros e busca avan√ßada
- [ ] 9.4 Agrega√ß√µes e estat√≠sticas
- [ ] 9.5 Documenta√ß√£o da API (Swagger/OpenAPI)
- [ ] 9.6 Sistema de autentica√ß√£o

### 10. Monitoramento e Logs (Story Points: 13)
- [ ] 10.1 Implementar logging estruturado
- [ ] 10.2 Dashboard de monitoramento
- [ ] 10.3 M√©tricas de performance
- [ ] 10.4 Sistema de alertas
- [ ] 10.5 Relat√≥rios autom√°ticos

### 11. Intelig√™ncia e An√°lise (Story Points: 21)
- [ ] 11.1 An√°lise de tend√™ncias salariais
- [ ] 11.2 Skills mais demandadas
- [ ] 11.3 An√°lise de empregadores
- [ ] 11.4 An√°lise geogr√°fica
- [ ] 11.5 Previs√£o de demanda
- [ ] 11.6 Exporta√ß√£o de relat√≥rios

### 12. Qualidade e Testes (Story Points: 13)
- [ ] 12.1 Testes unit√°rios (pytest)
- [ ] 12.2 Testes de integra√ß√£o
- [ ] 12.3 Testes de scrapers com dados mock
- [ ] 12.4 Code coverage > 80%
- [ ] 12.5 Linting e formata√ß√£o (black, flake8, mypy)

### 13. Documenta√ß√£o (Story Points: 8)
- [ ] 13.1 README detalhado
- [ ] 13.2 Documenta√ß√£o de arquitetura
- [ ] 13.3 Guia de contribui√ß√£o
- [ ] 13.4 Documenta√ß√£o de cada scraper
- [ ] 13.5 Runbook operacional

## ‚ö†Ô∏è Considera√ß√µes T√©cnicas e Legais

### Aspectos Legais
- ‚úÖ Respeitar robots.txt de cada site
- ‚úÖ Implementar rate limiting apropriado
- ‚úÖ Verificar Terms of Service de cada plataforma
- ‚úÖ Considerar uso de APIs oficiais quando dispon√≠veis
- ‚úÖ N√£o armazenar dados pessoais de candidatos (LGPD compliance)
- ‚úÖ Implementar opt-out se necess√°rio

### Desafios T√©cnicos
- Sites com prote√ß√£o anti-bot (Cloudflare, Captcha)
- Mudan√ßas frequentes em estruturas HTML
- Rate limiting agressivo
- Sites que requerem autentica√ß√£o
- JavaScript-heavy applications (SPAs)
- Dados n√£o estruturados

### Estrat√©gias de Mitiga√ß√£o
- Usar APIs oficiais sempre que poss√≠vel
- Implementar sistema robusto de error handling
- Cache inteligente para reduzir requests
- Monitoramento cont√≠nuo de quebras
- Versionamento de parsers
- Fallback strategies

## üìà M√©tricas de Sucesso

- **Cobertura**: Coletar de pelo menos 15 fontes diferentes
- **Volume**: Capturar m√≠nimo de 10.000 vagas ativas
- **Freshness**: Dados atualizados diariamente
- **Qualidade**: Taxa de erro < 5% na extra√ß√£o
- **Performance**: Coleta completa em < 2 horas
- **Uptime**: Sistema rodando 99%+ do tempo

## üöÄ Fases de Entrega

**MVP (Fase 1)** - 2-3 meses
- Setup b√°sico e 5 scrapers principais
- Banco de dados funcional
- Coleta agendada
- API b√°sica

**Fase 2** - 1-2 meses
- 10+ scrapers
- Sistema de enriquecimento
- Dashboard b√°sico
- An√°lises iniciais

**Fase 3** - 1-2 meses
- Todos os scrapers planejados
- Intelig√™ncia avan√ßada
- Machine Learning para classifica√ß√£o
- Sistema completo de monitoramento

## üìö Recursos e Refer√™ncias

- [Scrapy Documentation](https://docs.scrapy.org/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium Python](https://selenium-python.readthedocs.io/)
- [Apache Airflow](https://airflow.apache.org/)
- [FastAPI](https://fastapi.tiangolo.com/)

---

**Estimativa Total**: 241 Story Points (~6-8 meses com 1-2 desenvolvedores)
